{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66932677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1045 [00:13<3:59:18, 13.75s/it]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Define chunk size (number of rows to read and modify at a time)\n",
    "chunk_size = 64_000  # Adjust this according to your needs\n",
    "\n",
    "parquet_file = pq.ParquetFile('pairs_with_empty_similarity.parquet')\n",
    "\n",
    "total_rows = parquet_file.metadata.num_rows\n",
    "\n",
    "print(total_rows)\n",
    "\n",
    "output_writer = None\n",
    "\n",
    "rows = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "import pickle\n",
    "# read string_to_embedding.pickle\n",
    "with open('string_to_embedding.pickle', 'rb') as handle:\n",
    "    string_to_embedding = pickle.load(handle)\n",
    "\n",
    "try:\n",
    "    for batch in tqdm.tqdm(parquet_file.iter_batches(chunk_size), total=int(total_rows / chunk_size)):\n",
    "        df = batch.to_pandas()\n",
    "        # if the row's all have -1 similarity, skip\n",
    "        if df['similarity'].sum() != -1 * len(df):\n",
    "            # this means at least one has an overriden value\n",
    "            print('skipping')\n",
    "            continue\n",
    "        df['similarity'] = df.apply(lambda row: cosine_similarity([string_to_embedding[row['census_name'].lower()]], [string_to_embedding[row['deepdao_name'].lower()]])[0][0], axis=1)\n",
    "\n",
    "        modified_chunk_table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Append the modified chunk_table to the output Parquet file\n",
    "        if not output_writer:\n",
    "            # Create or open the output Parquet file\n",
    "            output_writer = pq.ParquetWriter(\n",
    "                'pairs_with_similarity.parquet',\n",
    "                modified_chunk_table.schema,\n",
    "            )\n",
    "        output_writer.write_table(modified_chunk_table)\n",
    "        \n",
    "        # count += 1\n",
    "        # if count > 3:\n",
    "            # print(len(rows))\n",
    "            # break\n",
    "            # FIXME: for dev\n",
    "except KeyboardInterrupt:\n",
    "    output_writer.close()\n",
    "    \n",
    "\n",
    "# Close the output Parquet writer\n",
    "output_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
